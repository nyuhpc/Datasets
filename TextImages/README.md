## Data Source

Dataset is generated by Visual Geometry Group of University of Oxford
This is synthetically generated dataset which found to be sufficient for training text recognition on real-world images
This dataset consists of 9 million images covering 90k English words, and includes the training, validation and test splits used in authors work.
(archived dataset is about 10 GB)

Please read more information here https://www.robots.ox.ac.uk/~vgg/data/text/


## File structure

File sturcture withing tar.gz:

mnt/ramdisk/max/90kDICT32px/<directory_as_number>/<directory_as_number>/<file_name_with_textLabel>.jpg     
mnt/ramdisk/max/90kDICT32px/<some_text_files_about_data>.txt

## Converting to other formats

This process is done using slurm session (it is expensive computation, and in SLURM sesssion SLURM_TMPDIR does not have the same limit on number of files, as /scratch)

### How data is stored:

*Images:* 

both lmdb and hfd5 contain images in binary jpg format. These objects can be read from any language: python, R, MATLAB, etc.

lmdb keys have 7 digits, for example image number 12 has key "0000012"

hdf5 dataset name is "images"
hdf5 elements start from 0. So 12th image would be accessed usign index 11

*Metadata:*

Metadata is stored in SQLite file

lmdb SQLite file schema: 
* key - lmdb key
* label - word extracted from filename
* path - path within original tar.gz archive

hdf5 SQLite file schema: 
* key - file number (starts from 0)
* label - word extracted from filename
* path - path within original tar.gz archive


## Files

### slurm sbatch script

slurm_send_writeAll - batch file for slurm job

### lmdb

Look at file write_jpg_lmdb_all.py

### hdf5

Look at file write_jpg_hdf5_all.py


## Some data

### how long it took

* lmdb: 1297 sec (~ 20 min)
* hdf5: 3523 sec (~ 1 hour)

### how big are the files

$ du -h lmdb/TextImages*
26G     lmdb/TextImages.lmdb
624M    lmdb/TextImages.sqlite

$ du -h hdf5/TextImages*
15G     hdf5/TextImages.hdf5
407M    hdf5/TextImages-hdf5.sqlite

### lmdb tree info

{'psize': 4096, 'depth': 4, 'branch_pages': 21444, 'leaf_pages': 4824460, 'overflow_pages': 1874847, 'entries': 8918999}


## Conda environment setup

```{bash}
module load anaconda3/5.3.1
conda create -p $(pwd)/penv python=3.7
conda activate ...
conda install -y pillow hdf5 h5py matplotlib numpy pathlib lmdb python-lmdb sqlite pandas
```
